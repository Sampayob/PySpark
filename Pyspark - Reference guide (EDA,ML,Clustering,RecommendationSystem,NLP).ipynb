{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyspark Dataframe: Reference Guide: EDA,ML,Clustering,Recommendation Systems,NLP, SparkStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating SparkSession and load .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Test').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('E:\\\\SparkScala\\\\advertising.csv',inferSchema=True, header=True)\n",
    "#df = spark.read.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+---+-----------+--------------------+--------------------+-----------------+----+--------------------+-------------------+-------------+\n",
      "|Daily Time Spent on Site|Age|Area Income|Daily Internet Usage|       Ad Topic Line|             City|Male|             Country|          Timestamp|Clicked on Ad|\n",
      "+------------------------+---+-----------+--------------------+--------------------+-----------------+----+--------------------+-------------------+-------------+\n",
      "|                   68.95| 35|    61833.9|              256.09|Cloned 5thgenerat...|      Wrightburgh|   0|             Tunisia|2016-03-27 00:53:11|            0|\n",
      "|                   80.23| 31|   68441.85|              193.77|Monitored nationa...|        West Jodi|   1|               Nauru|2016-04-04 01:39:02|            0|\n",
      "|                   69.47| 26|   59785.94|               236.5|Organic bottom-li...|         Davidton|   0|          San Marino|2016-03-13 20:35:42|            0|\n",
      "|                   74.15| 29|   54806.18|              245.89|Triple-buffered r...|   West Terrifurt|   1|               Italy|2016-01-10 02:31:19|            0|\n",
      "|                   68.37| 35|   73889.99|              225.58|Robust logistical...|     South Manuel|   0|             Iceland|2016-06-03 03:36:18|            0|\n",
      "|                   59.99| 23|   59761.56|              226.74|Sharable client-d...|        Jamieberg|   1|              Norway|2016-05-19 14:30:17|            0|\n",
      "|                   88.91| 33|   53852.85|              208.36|Enhanced dedicate...|      Brandonstad|   0|             Myanmar|2016-01-28 20:59:32|            0|\n",
      "|                    66.0| 48|   24593.33|              131.76|Reactive local ch...| Port Jefferybury|   1|           Australia|2016-03-07 01:40:15|            1|\n",
      "|                   74.53| 30|    68862.0|              221.51|Configurable cohe...|       West Colin|   1|             Grenada|2016-04-18 09:33:42|            0|\n",
      "|                   69.88| 20|   55642.32|              183.82|Mandatory homogen...|       Ramirezton|   1|               Ghana|2016-07-11 01:42:51|            0|\n",
      "|                   47.64| 49|   45632.51|              122.02|Centralized neutr...|  West Brandonton|   0|               Qatar|2016-03-16 20:19:01|            1|\n",
      "|                   83.07| 37|   62491.01|              230.87|Team-oriented gri...|East Theresashire|   1|             Burundi|2016-05-08 08:10:10|            0|\n",
      "|                   69.57| 48|   51636.92|              113.12|Centralized conte...|   West Katiefurt|   1|               Egypt|2016-06-03 01:14:41|            1|\n",
      "|                   79.52| 24|   51739.63|              214.23|Synergistic fresh...|       North Tara|   0|Bosnia and Herzeg...|2016-04-20 21:49:22|            0|\n",
      "|                   42.95| 33|    30976.0|              143.56|Grass-roots coher...|     West William|   0|            Barbados|2016-03-24 09:31:49|            1|\n",
      "|                   63.45| 23|   52182.23|              140.64|Persistent demand...|   New Travistown|   1|               Spain|2016-03-09 03:41:30|            1|\n",
      "|                   55.39| 37|   23936.86|              129.41|Customizable mult...|   West Dylanberg|   0|Palestinian Terri...|2016-01-30 19:20:41|            1|\n",
      "|                   82.03| 41|   71511.08|              187.53|Intuitive dynamic...|      Pruittmouth|   0|         Afghanistan|2016-05-02 07:00:58|            0|\n",
      "|                    54.7| 36|   31087.54|              118.39|Grass-roots solut...|      Jessicastad|   1|British Indian Oc...|2016-02-13 07:53:55|            1|\n",
      "|                   74.58| 40|   23821.72|              135.51|Advanced 24/7 pro...|       Millertown|   1|  Russian Federation|2016-02-27 04:43:07|            1|\n",
      "+------------------------+---+-----------+--------------------+--------------------+-----------------+----+--------------------+-------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10)\n"
     ]
    }
   ],
   "source": [
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+---+-----------+--------------------+--------------------+-----------------+----+--------------------+-------------------+-------------+\n",
      "|Daily Time Spent on Site|Age|Area Income|Daily Internet Usage|       Ad Topic Line|             City|Male|             Country|          Timestamp|Clicked on Ad|\n",
      "+------------------------+---+-----------+--------------------+--------------------+-----------------+----+--------------------+-------------------+-------------+\n",
      "|                   84.59| 35|   60015.57|              226.54|Streamlined non-v...|      Lake Nicole|   1|            Cameroon|2016-03-18 13:22:35|            0|\n",
      "|                   67.64| 35|   51473.28|              267.01|Programmable asym...|    Phelpschester|   1|                Peru|2016-07-02 20:23:15|            0|\n",
      "|                   49.81| 35|   57009.76|              120.06|Seamless real-tim...|     Ramirezhaven|   1|       Faroe Islands|2016-01-05 04:18:46|            1|\n",
      "|                   76.32| 35|   74166.24|              195.31|Networked foregro...| East Michaeltown|   1|               Korea|2016-06-23 00:16:02|            0|\n",
      "|                   59.21| 35|   73347.67|              144.62|Managed disinterm...|  Lake Beckyburgh|   1|       Liechtenstein|2016-02-02 08:55:26|            1|\n",
      "|                   53.38| 35|   60803.37|              120.06| Total local synergy|    Alexanderview|   1|       French Guiana|2016-03-30 01:05:34|            1|\n",
      "|                   88.89| 35|   50439.49|               218.8|Balanced 4thgener...|East Deborahhaven|   1|        Cook Islands|2016-07-17 18:55:38|            0|\n",
      "|                   66.88| 35|   62790.96|              119.47|Polarized bifurca...|     Russellville|   1|           Indonesia|2016-03-23 19:58:15|            1|\n",
      "|                   73.89| 35|   70495.64|              229.99|Multi-channeled a...|    Dustinborough|   1|             Somalia|2016-06-26 17:25:55|            0|\n",
      "|                   44.73| 35|   55316.97|              127.56|Object-based neut...|  North Lauraland|   1|                Guam|2016-01-27 18:25:42|            1|\n",
      "|                   50.18| 35|   63006.14|              127.82|Customizable hybr...|      Sandrashire|   1|             Grenada|2016-06-20 04:24:41|            1|\n",
      "|                   87.97| 35|   48918.55|              149.25|Secured secondary...|   Port Brianfort|   1|              France|2016-03-24 05:38:01|            0|\n",
      "|                   87.35| 35|    58114.3|              158.29|Cloned dedicated ...|      Carsonshire|   1|           Gibraltar|2016-01-26 02:47:17|            0|\n",
      "|                   39.96| 35|   53898.89|              138.52|Phased leadingedg...|      Kingchester|   1|    Pitcairn Islands|2016-02-28 23:21:22|            1|\n",
      "|                   61.87| 35|   66629.61|               250.2|Multi-channeled a...|   North Johnside|   1|French Southern T...|2016-07-10 19:15:52|            0|\n",
      "|                   48.86| 35|    62463.7|              128.37|Universal empower...|    Elizabethbury|   1|        Saint Martin|2016-01-04 00:44:57|            1|\n",
      "|                   43.16| 35|   25371.52|              156.11|Implemented didac...|     Williamsport|   1|    Marshall Islands|2016-07-06 03:40:17|            1|\n",
      "|                   35.11| 35|    47638.3|              158.03|Extended analyzin...|   North Jonathan|   1|            Anguilla|2016-03-11 13:07:30|            1|\n",
      "+------------------------+---+-----------+--------------------+--------------------+-----------------+----+--------------------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.filter(\"column == 10\").select(['column1', 'column2']).show()\n",
    "#df.filter(df[\"column\"] == 10).select(['column1', 'column2']).show()\n",
    "\n",
    "df.filter( (df[\"Age\"] == 35) & (df[\"Male\"] == 1) ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter as Dict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Daily Time Spent on Site': 84.59,\n",
       " 'Age': 35,\n",
       " 'Area Income': 60015.57,\n",
       " 'Daily Internet Usage': 226.54,\n",
       " 'Ad Topic Line': 'Streamlined non-volatile analyzer',\n",
       " 'City': 'Lake Nicole',\n",
       " 'Male': 1,\n",
       " 'Country': 'Cameroon',\n",
       " 'Timestamp': '2016-03-18 13:22:35',\n",
       " 'Clicked on Ad': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = df.filter( (df[\"Age\"] == 35) & (df[\"Male\"] == 1) ).collect() \n",
    "row = result[0]\n",
    "row.asDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**New  Column from another**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'length(Area Income)'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import length\n",
    "\n",
    "length(df['Area Income'])\n",
    "         \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|Area Income|Area Income*2|\n",
      "+-----------+-------------+\n",
      "|    61833.9|     123667.8|\n",
      "|   68441.85|     136883.7|\n",
      "|   59785.94|    119571.88|\n",
      "|   54806.18|    109612.36|\n",
      "|   73889.99|    147779.98|\n",
      "|   59761.56|    119523.12|\n",
      "|   53852.85|     107705.7|\n",
      "|   24593.33|     49186.66|\n",
      "|    68862.0|     137724.0|\n",
      "|   55642.32|    111284.64|\n",
      "|   45632.51|     91265.02|\n",
      "|   62491.01|    124982.02|\n",
      "|   51636.92|    103273.84|\n",
      "|   51739.63|    103479.26|\n",
      "|    30976.0|      61952.0|\n",
      "|   52182.23|    104364.46|\n",
      "|   23936.86|     47873.72|\n",
      "|   71511.08|    143022.16|\n",
      "|   31087.54|     62175.08|\n",
      "|   23821.72|     47643.44|\n",
      "+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('Area Income*2',df['Area Income']*2).select(['Area Income', 'Area Income*2']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rename Column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Daily Time Spent on Site: double, Age: int, Area Income: double, Daily Internet Usage: double, Ad Topic Line: string, City: string, Male: int, Country: string, Timestamp: string, Clicked on Ad: int]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumnRenamed('Area Income*2','Area IncomeDouble')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Querys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Temporay SQL view**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+---+-----------+--------------------+--------------------+-----------------+----+--------------------+-------------------+-------------+\n",
      "|Daily Time Spent on Site|Age|Area Income|Daily Internet Usage|       Ad Topic Line|             City|Male|             Country|          Timestamp|Clicked on Ad|\n",
      "+------------------------+---+-----------+--------------------+--------------------+-----------------+----+--------------------+-------------------+-------------+\n",
      "|                   68.95| 35|    61833.9|              256.09|Cloned 5thgenerat...|      Wrightburgh|   0|             Tunisia|2016-03-27 00:53:11|            0|\n",
      "|                   80.23| 31|   68441.85|              193.77|Monitored nationa...|        West Jodi|   1|               Nauru|2016-04-04 01:39:02|            0|\n",
      "|                   69.47| 26|   59785.94|               236.5|Organic bottom-li...|         Davidton|   0|          San Marino|2016-03-13 20:35:42|            0|\n",
      "|                   74.15| 29|   54806.18|              245.89|Triple-buffered r...|   West Terrifurt|   1|               Italy|2016-01-10 02:31:19|            0|\n",
      "|                   68.37| 35|   73889.99|              225.58|Robust logistical...|     South Manuel|   0|             Iceland|2016-06-03 03:36:18|            0|\n",
      "|                   59.99| 23|   59761.56|              226.74|Sharable client-d...|        Jamieberg|   1|              Norway|2016-05-19 14:30:17|            0|\n",
      "|                   88.91| 33|   53852.85|              208.36|Enhanced dedicate...|      Brandonstad|   0|             Myanmar|2016-01-28 20:59:32|            0|\n",
      "|                    66.0| 48|   24593.33|              131.76|Reactive local ch...| Port Jefferybury|   1|           Australia|2016-03-07 01:40:15|            1|\n",
      "|                   74.53| 30|    68862.0|              221.51|Configurable cohe...|       West Colin|   1|             Grenada|2016-04-18 09:33:42|            0|\n",
      "|                   69.88| 20|   55642.32|              183.82|Mandatory homogen...|       Ramirezton|   1|               Ghana|2016-07-11 01:42:51|            0|\n",
      "|                   47.64| 49|   45632.51|              122.02|Centralized neutr...|  West Brandonton|   0|               Qatar|2016-03-16 20:19:01|            1|\n",
      "|                   83.07| 37|   62491.01|              230.87|Team-oriented gri...|East Theresashire|   1|             Burundi|2016-05-08 08:10:10|            0|\n",
      "|                   69.57| 48|   51636.92|              113.12|Centralized conte...|   West Katiefurt|   1|               Egypt|2016-06-03 01:14:41|            1|\n",
      "|                   79.52| 24|   51739.63|              214.23|Synergistic fresh...|       North Tara|   0|Bosnia and Herzeg...|2016-04-20 21:49:22|            0|\n",
      "|                   42.95| 33|    30976.0|              143.56|Grass-roots coher...|     West William|   0|            Barbados|2016-03-24 09:31:49|            1|\n",
      "|                   63.45| 23|   52182.23|              140.64|Persistent demand...|   New Travistown|   1|               Spain|2016-03-09 03:41:30|            1|\n",
      "|                   55.39| 37|   23936.86|              129.41|Customizable mult...|   West Dylanberg|   0|Palestinian Terri...|2016-01-30 19:20:41|            1|\n",
      "|                   82.03| 41|   71511.08|              187.53|Intuitive dynamic...|      Pruittmouth|   0|         Afghanistan|2016-05-02 07:00:58|            0|\n",
      "|                    54.7| 36|   31087.54|              118.39|Grass-roots solut...|      Jessicastad|   1|British Indian Oc...|2016-02-13 07:53:55|            1|\n",
      "|                   74.58| 40|   23821.72|              135.51|Advanced 24/7 pro...|       Millertown|   1|  Russian Federation|2016-02-27 04:43:07|            1|\n",
      "+------------------------+---+-----------+--------------------+--------------------+-----------------+----+--------------------+-------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM test\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+---+-----------+--------------------+--------------------+----------------+----+-------------+-------------------+-------------+\n",
      "|Daily Time Spent on Site|Age|Area Income|Daily Internet Usage|       Ad Topic Line|            City|Male|      Country|          Timestamp|Clicked on Ad|\n",
      "+------------------------+---+-----------+--------------------+--------------------+----------------+----+-------------+-------------------+-------------+\n",
      "|                   78.76| 24|   46422.76|              219.98|Reactive interact...|       Joanntown|   1|      Austria|2016-01-08 08:08:47|            0|\n",
      "|                   87.98| 38|   56637.59|              222.11|Focused scalable ...|    West Roytown|   1|     Cambodia|2016-03-31 10:44:46|            0|\n",
      "|                   42.05| 51|   28357.27|              174.55|Configurable 24/7...|West Eduardotown|   1|       Canada|2016-06-20 14:20:52|            1|\n",
      "|                   69.11| 42|   73608.99|              231.48|Face-to-face miss...|       Novaktown|   1|Faroe Islands|2016-04-21 12:34:28|            0|\n",
      "|                   75.15| 33|   71296.67|              219.49|Synchronized zero...|       Smithtown|   1|       France|2016-06-19 18:19:38|            0|\n",
      "|                   54.97| 31|   51900.03|              116.38|Virtual bifurcate...|    Robinsontown|   1|       France|2016-07-07 12:17:33|            1|\n",
      "|                   35.65| 40|   31265.75|              172.58|Seamless intangib...|  North Johntown|   1|      Georgia|2016-05-10 07:22:37|            1|\n",
      "|                   40.17| 26|   47391.95|              171.31|Visionary asymmet...| South Lauratown|   1|        Japan|2016-02-01 14:37:34|            1|\n",
      "|                    37.0| 48|   36782.38|              158.22|Function-based co...|    Jonathantown|   1|        Kenya|2016-03-24 06:36:52|            1|\n",
      "|                   76.32| 35|   74166.24|              195.31|Networked foregro...|East Michaeltown|   1|        Korea|2016-06-23 00:16:02|            0|\n",
      "+------------------------+---+-----------+--------------------+--------------------+----------------+----+-------------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM test WHERE Male = 1 AND City LIKE '%town%' ORDER BY Country LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### groupBy and agg function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x1b6636d1ac8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy(\"City\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|             City|count|\n",
      "+-----------------+-----+\n",
      "|      Wrightburgh|    2|\n",
      "|        West Jodi|    1|\n",
      "|         Davidton|    1|\n",
      "|   West Terrifurt|    1|\n",
      "|     South Manuel|    1|\n",
      "|        Jamieberg|    1|\n",
      "|      Brandonstad|    1|\n",
      "| Port Jefferybury|    1|\n",
      "|       West Colin|    1|\n",
      "|       Ramirezton|    1|\n",
      "|  West Brandonton|    1|\n",
      "|East Theresashire|    1|\n",
      "|   West Katiefurt|    1|\n",
      "|       North Tara|    1|\n",
      "|     West William|    1|\n",
      "|   New Travistown|    1|\n",
      "|   West Dylanberg|    1|\n",
      "|      Pruittmouth|    1|\n",
      "|      Jessicastad|    1|\n",
      "|       Millertown|    2|\n",
      "+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"City\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|   sum(Area Income)|\n",
      "+-------------------+\n",
      "|5.500000008000003E7|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({'Area Income':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+\n",
      "|             City|  sum(Area Income)|\n",
      "+-----------------+------------------+\n",
      "|      Wrightburgh|127859.01000000001|\n",
      "|        West Jodi|          68441.85|\n",
      "|         Davidton|          59785.94|\n",
      "|   West Terrifurt|          54806.18|\n",
      "|     South Manuel|          73889.99|\n",
      "|        Jamieberg|          59761.56|\n",
      "|      Brandonstad|          53852.85|\n",
      "| Port Jefferybury|          24593.33|\n",
      "|       West Colin|           68862.0|\n",
      "|       Ramirezton|          55642.32|\n",
      "|  West Brandonton|          45632.51|\n",
      "|East Theresashire|          62491.01|\n",
      "|   West Katiefurt|          51636.92|\n",
      "|       North Tara|          51739.63|\n",
      "|     West William|           30976.0|\n",
      "|   New Travistown|          52182.23|\n",
      "|   West Dylanberg|          23936.86|\n",
      "|      Pruittmouth|          71511.08|\n",
      "|      Jessicastad|          31087.54|\n",
      "|       Millertown|          87195.42|\n",
      "+-----------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "group_data = df.groupBy(\"City\")\n",
    "group_data.agg({'Area Income':'sum'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|  Area Income AVG|\n",
      "+-----------------+\n",
      "|55000.00008000003|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct,avg,stddev\n",
    "\n",
    "df.select(avg('Area Income').alias('Area Income AVG')).show()\n",
    "\n",
    "# alias: gave a new name to the created column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selecting number of decimals**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|Area Income AVG|\n",
      "+---------------+\n",
      "|      55,000.00|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import format_number\n",
    "\n",
    "#format_number: round numbers\n",
    "\n",
    "area_avg = df.select(avg('Area Income').alias('Area Income AVG'))\n",
    "area_avg.select(format_number('Area Income AVG',2).alias('Area Income AVG')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### orderBy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataframe style**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+\n",
      "|Area Income|               City|\n",
      "+-----------+-------------------+\n",
      "|    79484.8|        Edwardmouth|\n",
      "|   79332.33|           Mataberg|\n",
      "|   78520.99|        East Ronald|\n",
      "|    78119.5|  Port Whitneyhaven|\n",
      "|   78092.95|North Isabellaville|\n",
      "|   77988.71|    West Robertside|\n",
      "|   77871.75|   North Jeremyport|\n",
      "|   77567.85|       Port Destiny|\n",
      "|   77460.07|      Kimberlyhaven|\n",
      "|   77220.42|   Lake Jenniferton|\n",
      "|   77143.61|         Staceyfort|\n",
      "|   76984.21|        Lake Hailey|\n",
      "|   76893.84|         Garciaview|\n",
      "|   76560.59|         Jordantown|\n",
      "|   76480.16|      New Jamestown|\n",
      "|    76435.3|          Kellytown|\n",
      "|   76408.19|  Lake Jasonchester|\n",
      "|   76368.31|        Timothyfurt|\n",
      "|   76246.96|     North Mercedes|\n",
      "|   76003.47|        North Randy|\n",
      "+-----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(df['Area Income'].desc()).select(['Area Income', 'City']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL style**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+\n",
      "|Area Income|               City|\n",
      "+-----------+-------------------+\n",
      "|    79484.8|        Edwardmouth|\n",
      "|   79332.33|           Mataberg|\n",
      "|   78520.99|        East Ronald|\n",
      "|    78119.5|  Port Whitneyhaven|\n",
      "|   78092.95|North Isabellaville|\n",
      "|   77988.71|    West Robertside|\n",
      "|   77871.75|   North Jeremyport|\n",
      "|   77567.85|       Port Destiny|\n",
      "|   77460.07|      Kimberlyhaven|\n",
      "|   77220.42|   Lake Jenniferton|\n",
      "|   77143.61|         Staceyfort|\n",
      "|   76984.21|        Lake Hailey|\n",
      "|   76893.84|         Garciaview|\n",
      "|   76560.59|         Jordantown|\n",
      "|   76480.16|      New Jamestown|\n",
      "|    76435.3|          Kellytown|\n",
      "|   76408.19|  Lake Jasonchester|\n",
      "|   76368.31|        Timothyfurt|\n",
      "|   76246.96|     North Mercedes|\n",
      "|   76003.47|        North Randy|\n",
      "+-----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('orderBytest')\n",
    "spark.sql(\"SELECT `Area Income`, City FROM orderBytest ORDER BY `Area Income` DESC\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate numeric and categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = df.select([t[0] for t in df.dtypes if (t[1] == 'int') or (t[1] == 'double')])\n",
    "df_cat = df.select([t[0] for t in df.dtypes if t[1] == 'string'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('E:\\\\SparkScala\\\\ContainsNull.csv',inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search for missings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id  Name  Sales\n",
       "0  0.0   2.0    2.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_null = df.select(*(F.sum(F.col(c).isNull().cast(\"Double\")).alias(c) for c in df.columns)).toPandas()\n",
    "df_null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop rows with all null values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how='all').show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop na values of selected columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(subset=['Sales']).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fill null values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|  0.0|\n",
      "|emp2| null|  0.0|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|  NaN|\n",
      "|emp2| null|  NaN|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(float('nan')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "|  Id|   Name|Sales|\n",
      "+----+-------+-----+\n",
      "|emp1|   John| null|\n",
      "|emp2|No Name| null|\n",
      "|emp3|No Name|345.0|\n",
      "|emp4|  Cindy|456.0|\n",
      "+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill('No Name', subset = ['Name']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fill with the mean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(avg(Sales)=400.5)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "mean_val = df.select(mean(df['Sales'])).collect()\n",
    "mean_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400.5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_val[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|400.5|\n",
      "|emp2| null|400.5|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mean_sales = mean_val[0][0]\n",
    "\n",
    "df.na.fill(mean_sales,['Sales']).show()\n",
    "#in one line: df.na.fill(df.select(mean(df['Sales'])).collect()[0][0],['Sales']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dates and Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      Date|              Open|\n",
      "+----------+------------------+\n",
      "|2010-01-04|        213.429998|\n",
      "|2010-01-05|        214.599998|\n",
      "|2010-01-06|        214.379993|\n",
      "|2010-01-07|            211.75|\n",
      "|2010-01-08|        210.299994|\n",
      "|2010-01-11|212.79999700000002|\n",
      "|2010-01-12|209.18999499999998|\n",
      "|2010-01-13|        207.870005|\n",
      "|2010-01-14|210.11000299999998|\n",
      "|2010-01-15|210.92999500000002|\n",
      "|2010-01-19|        208.330002|\n",
      "|2010-01-20|        214.910006|\n",
      "|2010-01-21|        212.079994|\n",
      "|2010-01-22|206.78000600000001|\n",
      "|2010-01-25|202.51000200000001|\n",
      "|2010-01-26|205.95000100000001|\n",
      "|2010-01-27|        206.849995|\n",
      "|2010-01-28|        204.930004|\n",
      "|2010-01-29|        201.079996|\n",
      "|2010-02-01|192.36999699999998|\n",
      "+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('E:\\\\SparkScala\\\\appl_stock.csv',inferSchema=True, header=True)\n",
    "\n",
    "df.select(['Date','Open']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (dayofmonth, hour,dayofyear, dayofyear, weekofyear, format_number,date_format, year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|dayofmonth(Date)|\n",
      "+----------------+\n",
      "|               4|\n",
      "|               5|\n",
      "|               6|\n",
      "|               7|\n",
      "|               8|\n",
      "|              11|\n",
      "|              12|\n",
      "|              13|\n",
      "|              14|\n",
      "|              15|\n",
      "|              19|\n",
      "|              20|\n",
      "|              21|\n",
      "|              22|\n",
      "|              25|\n",
      "|              26|\n",
      "|              27|\n",
      "|              28|\n",
      "|              29|\n",
      "|               1|\n",
      "+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(dayofmonth(df['Date'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|weekofyear(Date)|\n",
      "+----------------+\n",
      "|               1|\n",
      "|               1|\n",
      "|               1|\n",
      "|               1|\n",
      "|               1|\n",
      "|               2|\n",
      "|               2|\n",
      "|               2|\n",
      "|               2|\n",
      "|               2|\n",
      "|               3|\n",
      "|               3|\n",
      "|               3|\n",
      "|               3|\n",
      "|               4|\n",
      "|               4|\n",
      "|               4|\n",
      "|               4|\n",
      "|               4|\n",
      "|               5|\n",
      "+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(weekofyear(df['Date'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|Year|        avg(Close)|\n",
      "+----+------------------+\n",
      "|2010| 259.8424600000002|\n",
      "|2011|364.00432532142867|\n",
      "|2012| 576.0497195640002|\n",
      "|2013| 472.6348802857143|\n",
      "|2014| 295.4023416507935|\n",
      "|2015|120.03999980555547|\n",
      "|2016|104.60400786904763|\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.withColumn('Year',year(df['Date']))\n",
    "result = newdf.groupby('Year').mean().select([\"Year\",\"avg(Close)\"])\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|Year|Avg Close|\n",
      "+----+---------+\n",
      "|2010|   259.84|\n",
      "|2011|   364.00|\n",
      "|2012|   576.05|\n",
      "|2013|   472.63|\n",
      "|2014|   295.40|\n",
      "|2015|   120.04|\n",
      "|2016|   104.60|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result2 = result.withColumnRenamed(\"avg(Close)\",\"Avg Close\")\n",
    "\n",
    "result2.select([\"Year\",format_number(\"Avg Close\",2).alias(\"Avg Close\")]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('lr_ecommerce_customers').getOrCreate()\n",
    "\n",
    "data = spark.read.csv('E:\\\\SparkScala\\\\Ecommerce_Customers.csv',inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Avatar: string (nullable = true)\n",
      " |-- Avg Session Length: double (nullable = true)\n",
      " |-- Time on App: double (nullable = true)\n",
      " |-- Time on Website: double (nullable = true)\n",
      " |-- Length of Membership: double (nullable = true)\n",
      " |-- Yearly Amount Spent: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mstephenson@fernandez.com\n",
      "835 Frank TunnelWrightmouth, MI 82180-9605\n",
      "Violet\n",
      "34.49726772511229\n",
      "12.65565114916675\n",
      "39.57766801952616\n",
      "4.0826206329529615\n",
      "587.9510539684005\n"
     ]
    }
   ],
   "source": [
    "for item in data.head(1)[0]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing data for ML: label and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Email',\n",
       " 'Address',\n",
       " 'Avatar',\n",
       " 'Avg Session Length',\n",
       " 'Time on App',\n",
       " 'Time on Website',\n",
       " 'Length of Membership',\n",
       " 'Yearly Amount Spent']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['Avg Session Length','Time on App','Time on Website','Length of Membership']\n",
    "                           ,outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Avatar: string (nullable = true)\n",
      " |-- Avg Session Length: double (nullable = true)\n",
      " |-- Time on App: double (nullable = true)\n",
      " |-- Time on Website: double (nullable = true)\n",
      " |-- Length of Membership: double (nullable = true)\n",
      " |-- Yearly Amount Spent: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = assembler.transform(data)\n",
    "output.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|            features|Yearly Amount Spent|\n",
      "+--------------------+-------------------+\n",
      "|[34.4972677251122...|  587.9510539684005|\n",
      "|[31.9262720263601...|  392.2049334443264|\n",
      "|[33.0009147556426...| 487.54750486747207|\n",
      "|[34.3055566297555...|  581.8523440352177|\n",
      "|[33.3306725236463...|  599.4060920457634|\n",
      "|[33.8710378793419...|   637.102447915074|\n",
      "|[32.0215955013870...|  521.5721747578274|\n",
      "|[32.7391429383803...|  549.9041461052942|\n",
      "|[33.9877728956856...|  570.2004089636196|\n",
      "|[31.9365486184489...|  427.1993848953282|\n",
      "|[33.9925727749537...|  492.6060127179966|\n",
      "|[33.8793608248049...|  522.3374046069357|\n",
      "|[29.5324289670579...|  408.6403510726275|\n",
      "|[33.1903340437226...|  573.4158673313865|\n",
      "|[32.3879758531538...|  470.4527333009554|\n",
      "|[30.7377203726281...|  461.7807421962299|\n",
      "|[32.1253868972878...| 457.84769594494855|\n",
      "|[32.3388993230671...| 407.70454754954415|\n",
      "|[32.1878120459321...|  452.3156754800354|\n",
      "|[32.6178560628234...|   605.061038804892|\n",
      "+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data = output.select('features','Yearly Amount Spent')\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Linear Regression object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression (featuresCol = 'features', labelCol = 'Yearly Amount Spent', predictionCol = 'prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([25.6839, 38.8674, 0.6419, 61.6344])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1059.56019185072"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrModel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.728296113196304"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_summary = lrModel.summary\n",
    "training_summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = lrModel.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "| -4.966921858349053|\n",
      "| 10.521945640504896|\n",
      "| -17.01456302984309|\n",
      "| -4.343074215920467|\n",
      "|-13.015774219660841|\n",
      "|-1.1233461231821593|\n",
      "|-3.7184452482358665|\n",
      "| 3.7746043674629846|\n",
      "| -4.425259598521279|\n",
      "| -6.262894048511953|\n",
      "|-14.631307920429435|\n",
      "| 17.038919635449304|\n",
      "|  6.179346828807354|\n",
      "|-0.8930956727532475|\n",
      "| 2.6319959197724643|\n",
      "|-18.143341026325118|\n",
      "| 1.0250391304259665|\n",
      "|  8.284164225009647|\n",
      "|-10.666177572081665|\n",
      "|-17.379468433818204|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results.residuals.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.394822527606221"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.983209533997304"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_data = test_data.select('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[30.4925366965402...|\n",
      "|[30.7377203726281...|\n",
      "|[30.8162006488763...|\n",
      "|[30.8364326747734...|\n",
      "|[31.0662181616375...|\n",
      "|[31.2606468698795...|\n",
      "|[31.2681042107507...|\n",
      "|[31.3584771924370...|\n",
      "|[31.4252268808548...|\n",
      "|[31.5147378578019...|\n",
      "|[31.5741380228732...|\n",
      "|[31.6098395733896...|\n",
      "|[31.6548096756927...|\n",
      "|[31.6610498227460...|\n",
      "|[31.7366356860502...|\n",
      "|[31.8164283341993...|\n",
      "|[31.8293464559211...|\n",
      "|[31.8512531286083...|\n",
      "|[31.8648325480987...|\n",
      "|[31.9048571310136...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unlabeled_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|            features|        prediction|\n",
      "+--------------------+------------------+\n",
      "|[30.4925366965402...| 287.4381675782636|\n",
      "|[30.7377203726281...|  451.258796555725|\n",
      "|[30.8162006488763...| 283.1009039783121|\n",
      "|[30.8364326747734...| 471.8449746429101|\n",
      "|[31.0662181616375...| 461.9490674273352|\n",
      "|[31.2606468698795...|422.44997738013353|\n",
      "|[31.2681042107507...| 427.1889784220598|\n",
      "|[31.3584771924370...| 491.4013460820124|\n",
      "|[31.4252268808548...| 535.1919782532832|\n",
      "|[31.5147378578019...|496.07538204497337|\n",
      "|[31.5741380228732...| 559.0405800810163|\n",
      "|[31.6098395733896...|427.50663001565886|\n",
      "|[31.6548096756927...|469.08407689874116|\n",
      "|[31.6610498227460...| 417.2514492526541|\n",
      "|[31.7366356860502...|494.30145033575945|\n",
      "|[31.8164283341993...| 519.2658325299815|\n",
      "|[31.8293464559211...|  384.127298857549|\n",
      "|[31.8512531286083...|464.70808244178875|\n",
      "|[31.8648325480987...|450.55745804889534|\n",
      "|[31.9048571310136...|491.32932585663434|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = lrModel.transform(unlabeled_data)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('logr').getOrCreate()\n",
    "\n",
    "df = spark.read.csv('E:\\\\SparkScala\\\\heart.csv',inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- sex: integer (nullable = true)\n",
      " |-- cp: integer (nullable = true)\n",
      " |-- trestbps: integer (nullable = true)\n",
      " |-- chol: integer (nullable = true)\n",
      " |-- fbs: integer (nullable = true)\n",
      " |-- restecg: integer (nullable = true)\n",
      " |-- thalach: integer (nullable = true)\n",
      " |-- exang: integer (nullable = true)\n",
      " |-- oldpeak: double (nullable = true)\n",
      " |-- slope: integer (nullable = true)\n",
      " |-- ca: integer (nullable = true)\n",
      " |-- thal: integer (nullable = true)\n",
      " |-- target: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'sex',\n",
       " 'cp',\n",
       " 'trestbps',\n",
       " 'chol',\n",
       " 'fbs',\n",
       " 'restecg',\n",
       " 'thalach',\n",
       " 'exang',\n",
       " 'oldpeak',\n",
       " 'slope',\n",
       " 'ca',\n",
       " 'thal',\n",
       " 'target']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(age=63, sex=1, cp=3, trestbps=145, chol=233, fbs=1, restecg=0, thalach=150, exang=0, oldpeak=2.3, slope=0, ca=0, thal=1, target=1)\n",
      "/n\n",
      "Row(age=37, sex=1, cp=2, trestbps=130, chol=250, fbs=0, restecg=1, thalach=187, exang=0, oldpeak=3.5, slope=0, ca=0, thal=2, target=1)\n",
      "/n\n",
      "Row(age=41, sex=0, cp=1, trestbps=130, chol=204, fbs=0, restecg=0, thalach=172, exang=0, oldpeak=1.4, slope=2, ca=0, thal=2, target=1)\n",
      "/n\n",
      "Row(age=56, sex=1, cp=1, trestbps=120, chol=236, fbs=0, restecg=1, thalach=178, exang=0, oldpeak=0.8, slope=2, ca=0, thal=2, target=1)\n",
      "/n\n",
      "Row(age=57, sex=0, cp=0, trestbps=120, chol=354, fbs=0, restecg=1, thalach=163, exang=1, oldpeak=0.6, slope=2, ca=0, thal=2, target=1)\n",
      "/n\n"
     ]
    }
   ],
   "source": [
    "for item in df.head(5):\n",
    "    print(item)\n",
    "    print('/n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing data for ML: label and features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['age',\n",
    " 'sex',\n",
    " 'cp',\n",
    " 'trestbps',\n",
    " 'chol',\n",
    " 'fbs',\n",
    " 'restecg',\n",
    " 'thalach',\n",
    " 'exang',\n",
    " 'oldpeak',\n",
    " 'slope',\n",
    " 'ca',\n",
    " 'thal'],outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- sex: integer (nullable = true)\n",
      " |-- cp: integer (nullable = true)\n",
      " |-- trestbps: integer (nullable = true)\n",
      " |-- chol: integer (nullable = true)\n",
      " |-- fbs: integer (nullable = true)\n",
      " |-- restecg: integer (nullable = true)\n",
      " |-- thalach: integer (nullable = true)\n",
      " |-- exang: integer (nullable = true)\n",
      " |-- oldpeak: double (nullable = true)\n",
      " |-- slope: integer (nullable = true)\n",
      " |-- ca: integer (nullable = true)\n",
      " |-- thal: integer (nullable = true)\n",
      " |-- target: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = assembler.transform(df)\n",
    "output.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            features|target|\n",
      "+--------------------+------+\n",
      "|[63.0,1.0,3.0,145...|     1|\n",
      "|[37.0,1.0,2.0,130...|     1|\n",
      "|[41.0,0.0,1.0,130...|     1|\n",
      "|[56.0,1.0,1.0,120...|     1|\n",
      "|[57.0,0.0,0.0,120...|     1|\n",
      "|[57.0,1.0,0.0,140...|     1|\n",
      "|[56.0,0.0,1.0,140...|     1|\n",
      "|[44.0,1.0,1.0,120...|     1|\n",
      "|[52.0,1.0,2.0,172...|     1|\n",
      "|[57.0,1.0,2.0,150...|     1|\n",
      "|[54.0,1.0,0.0,140...|     1|\n",
      "|[48.0,0.0,2.0,130...|     1|\n",
      "|[49.0,1.0,1.0,130...|     1|\n",
      "|[64.0,1.0,3.0,110...|     1|\n",
      "|[58.0,0.0,3.0,150...|     1|\n",
      "|[50.0,0.0,2.0,120...|     1|\n",
      "|[58.0,0.0,2.0,120...|     1|\n",
      "|[66.0,0.0,3.0,150...|     1|\n",
      "|[43.0,1.0,0.0,150...|     1|\n",
      "|[69.0,0.0,3.0,140...|     1|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data = output.select('features','target')\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Logistic Regression object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression (labelCol = 'target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "logregModel = logreg.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-0.025, -1.4063, 1.0545, -0.0334, -0.0011, 0.5954, 0.6333, 0.0186, -0.5778, -0.9553, 0.2247, -0.7077, -0.8241])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logregModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.20561952395422"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logregModel .intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+\n",
      "|summary|            target|         prediction|\n",
      "+-------+------------------+-------------------+\n",
      "|  count|               219|                219|\n",
      "|   mean|0.5616438356164384|  0.589041095890411|\n",
      "| stddev| 0.497322225021984|0.49313497524305433|\n",
      "|    min|               0.0|                0.0|\n",
      "|    max|               1.0|                1.0|\n",
      "+-------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_summary = logregModel.summary\n",
    "training_summary.predictions.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictions and Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+--------------------+----------+\n",
      "|            features|target|       rawPrediction|         probability|prediction|\n",
      "+--------------------+------+--------------------+--------------------+----------+\n",
      "|(13,[0,1,3,4,7,10...|     1|[-1.7865646103620...|[0.14349442895241...|       1.0|\n",
      "|(13,[0,1,3,4,7,10...|     1|[-0.5992549991149...|[0.35451415672557...|       1.0|\n",
      "|(13,[0,3,4,7,9,11...|     0|[3.84687759780020...|[0.97909985623355...|       0.0|\n",
      "|(13,[0,3,4,7,10,1...|     1|[-1.7873140332805...|[0.14340234665081...|       1.0|\n",
      "|[29.0,1.0,1.0,130...|     1|[-3.1196863953224...|[0.04230247507723...|       1.0|\n",
      "|[37.0,0.0,2.0,120...|     1|[-5.7391564000637...|[0.00320716241245...|       1.0|\n",
      "|[38.0,1.0,2.0,138...|     1|[-0.9760162062247...|[0.27368297020338...|       1.0|\n",
      "|[40.0,1.0,0.0,110...|     0|[2.67713446245765...|[0.93566384197944...|       0.0|\n",
      "|[41.0,0.0,1.0,130...|     1|[-2.3292050968815...|[0.08873291762431...|       1.0|\n",
      "|[41.0,1.0,1.0,120...|     1|[-3.4666392646373...|[0.03027649649552...|       1.0|\n",
      "|[41.0,1.0,2.0,112...|     1|[-4.6285608538458...|[0.00967430134320...|       1.0|\n",
      "|[41.0,1.0,2.0,130...|     1|[-1.0937574477388...|[0.25091138704394...|       1.0|\n",
      "|[42.0,1.0,1.0,120...|     1|[-2.9147167494046...|[0.05143083804409...|       1.0|\n",
      "|[42.0,1.0,2.0,120...|     1|[-3.1844497000828...|[0.03975511989872...|       1.0|\n",
      "|[42.0,1.0,2.0,130...|     1|[-3.5400242178977...|[0.02819462439973...|       1.0|\n",
      "|[43.0,0.0,0.0,132...|     0|[2.22571199014161...|[0.90253481164102...|       0.0|\n",
      "|[43.0,1.0,0.0,132...|     0|[3.4569786933185,...|[0.96943858006143...|       0.0|\n",
      "|[44.0,1.0,0.0,112...|     0|[-0.5744415998046...|[0.36021258183198...|       1.0|\n",
      "|[44.0,1.0,2.0,120...|     1|[-4.1266489443751...|[0.01588060075483...|       1.0|\n",
      "|[44.0,1.0,2.0,130...|     1|[-3.0110298362899...|[0.04693006204239...|       1.0|\n",
      "+--------------------+------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "pred_and_labels = logregModel.evaluate(test_data)\n",
    "\n",
    "pred_and_labels.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction',labelCol='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auccuracy = my_eval.evaluate(pred_and_labels.predictions)\n",
    "auccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Methods (DecisionTree, GradientBoostingTree, RandomForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('tree').getOrCreate()\n",
    "\n",
    "df = spark.read.csv('E:\\\\SparkScala\\\\college.csv',inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- School: string (nullable = true)\n",
      " |-- Private: string (nullable = true)\n",
      " |-- Apps: integer (nullable = true)\n",
      " |-- Accept: integer (nullable = true)\n",
      " |-- Enroll: integer (nullable = true)\n",
      " |-- Top10perc: integer (nullable = true)\n",
      " |-- Top25perc: integer (nullable = true)\n",
      " |-- F_Undergrad: integer (nullable = true)\n",
      " |-- P_Undergrad: integer (nullable = true)\n",
      " |-- Outstate: integer (nullable = true)\n",
      " |-- Room_Board: integer (nullable = true)\n",
      " |-- Books: integer (nullable = true)\n",
      " |-- Personal: integer (nullable = true)\n",
      " |-- PhD: integer (nullable = true)\n",
      " |-- Terminal: integer (nullable = true)\n",
      " |-- S_F_Ratio: double (nullable = true)\n",
      " |-- perc_alumni: integer (nullable = true)\n",
      " |-- Expend: integer (nullable = true)\n",
      " |-- Grad_Rate: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(School='Abilene Christian University', Private='Yes', Apps=1660, Accept=1232, Enroll=721, Top10perc=23, Top25perc=52, F_Undergrad=2885, P_Undergrad=537, Outstate=7440, Room_Board=3300, Books=450, Personal=2200, PhD=70, Terminal=78, S_F_Ratio=18.1, perc_alumni=12, Expend=7041, Grad_Rate=60)\n",
      "/n\n",
      "Row(School='Adelphi University', Private='Yes', Apps=2186, Accept=1924, Enroll=512, Top10perc=16, Top25perc=29, F_Undergrad=2683, P_Undergrad=1227, Outstate=12280, Room_Board=6450, Books=750, Personal=1500, PhD=29, Terminal=30, S_F_Ratio=12.2, perc_alumni=16, Expend=10527, Grad_Rate=56)\n",
      "/n\n",
      "Row(School='Adrian College', Private='Yes', Apps=1428, Accept=1097, Enroll=336, Top10perc=22, Top25perc=50, F_Undergrad=1036, P_Undergrad=99, Outstate=11250, Room_Board=3750, Books=400, Personal=1165, PhD=53, Terminal=66, S_F_Ratio=12.9, perc_alumni=30, Expend=8735, Grad_Rate=54)\n",
      "/n\n"
     ]
    }
   ],
   "source": [
    "for item in df.head(3):\n",
    "    print(item)\n",
    "    print('/n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing data for ML: label and features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['School',\n",
       " 'Private',\n",
       " 'Apps',\n",
       " 'Accept',\n",
       " 'Enroll',\n",
       " 'Top10perc',\n",
       " 'Top25perc',\n",
       " 'F_Undergrad',\n",
       " 'P_Undergrad',\n",
       " 'Outstate',\n",
       " 'Room_Board',\n",
       " 'Books',\n",
       " 'Personal',\n",
       " 'PhD',\n",
       " 'Terminal',\n",
       " 'S_F_Ratio',\n",
       " 'perc_alumni',\n",
       " 'Expend',\n",
       " 'Grad_Rate']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['Apps',\n",
    " 'Accept',\n",
    " 'Enroll',\n",
    " 'Top10perc',\n",
    " 'Top25perc',\n",
    " 'F_Undergrad',\n",
    " 'P_Undergrad',\n",
    " 'Outstate',\n",
    " 'Room_Board',\n",
    " 'Books',\n",
    " 'Personal',\n",
    " 'PhD',\n",
    " 'Terminal',\n",
    " 'S_F_Ratio',\n",
    " 'perc_alumni',\n",
    " 'Expend',\n",
    " 'Grad_Rate'],outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- School: string (nullable = true)\n",
      " |-- Private: string (nullable = true)\n",
      " |-- Apps: integer (nullable = true)\n",
      " |-- Accept: integer (nullable = true)\n",
      " |-- Enroll: integer (nullable = true)\n",
      " |-- Top10perc: integer (nullable = true)\n",
      " |-- Top25perc: integer (nullable = true)\n",
      " |-- F_Undergrad: integer (nullable = true)\n",
      " |-- P_Undergrad: integer (nullable = true)\n",
      " |-- Outstate: integer (nullable = true)\n",
      " |-- Room_Board: integer (nullable = true)\n",
      " |-- Books: integer (nullable = true)\n",
      " |-- Personal: integer (nullable = true)\n",
      " |-- PhD: integer (nullable = true)\n",
      " |-- Terminal: integer (nullable = true)\n",
      " |-- S_F_Ratio: double (nullable = true)\n",
      " |-- perc_alumni: integer (nullable = true)\n",
      " |-- Expend: integer (nullable = true)\n",
      " |-- Grad_Rate: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = assembler.transform(df)\n",
    "output.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changed label column to binary\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol = 'Private', outputCol = 'PrivateIndex')\n",
    "\n",
    "output_fixed = indexer.fit(output).transform(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|            features|PrivateIndex|\n",
      "+--------------------+------------+\n",
      "|[1660.0,1232.0,72...|         0.0|\n",
      "|[2186.0,1924.0,51...|         0.0|\n",
      "|[1428.0,1097.0,33...|         0.0|\n",
      "|[417.0,349.0,137....|         0.0|\n",
      "|[193.0,146.0,55.0...|         0.0|\n",
      "|[587.0,479.0,158....|         0.0|\n",
      "|[353.0,340.0,103....|         0.0|\n",
      "|[1899.0,1720.0,48...|         0.0|\n",
      "|[1038.0,839.0,227...|         0.0|\n",
      "|[582.0,498.0,172....|         0.0|\n",
      "|[1732.0,1425.0,47...|         0.0|\n",
      "|[2652.0,1900.0,48...|         0.0|\n",
      "|[1179.0,780.0,290...|         0.0|\n",
      "|[1267.0,1080.0,38...|         0.0|\n",
      "|[494.0,313.0,157....|         0.0|\n",
      "|[1420.0,1093.0,22...|         0.0|\n",
      "|[4302.0,992.0,418...|         0.0|\n",
      "|[1216.0,908.0,423...|         0.0|\n",
      "|[1130.0,704.0,322...|         0.0|\n",
      "|[3540.0,2001.0,10...|         1.0|\n",
      "+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data = output_fixed.select('features','PrivateIndex')\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Tree methods objects/classifiers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import (DecisionTreeClassifier, GBTClassifier, RandomForestClassifier)\n",
    "\n",
    "# Use mostly defaults to make this comparison \"fair\"\n",
    "\n",
    "dtc = DecisionTreeClassifier(labelCol='PrivateIndex',featuresCol='features')\n",
    "rfc = RandomForestClassifier(numTrees = 100,labelCol='PrivateIndex',featuresCol='features')\n",
    "gbt = GBTClassifier(labelCol='PrivateIndex',featuresCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the models (its three models, so it might take some time)\n",
    "dtc_model = dtc.fit(train_data)\n",
    "rfc_model = rfc.fit(train_data)\n",
    "gbt_model = gbt.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(17, {1: 0.0166, 3: 0.0088, 4: 0.0078, 5: 0.3156, 6: 0.0511, 7: 0.5681, 14: 0.0182, 16: 0.0137})"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc_model.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(17, {0: 0.0337, 1: 0.0592, 2: 0.1149, 3: 0.0132, 4: 0.0057, 5: 0.1922, 6: 0.0664, 7: 0.2429, 8: 0.0591, 9: 0.0079, 10: 0.0093, 11: 0.0107, 12: 0.0126, 13: 0.0672, 14: 0.0312, 15: 0.0477, 16: 0.0261})"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_model.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(17, {0: 0.0271, 1: 0.008, 2: 0.0105, 3: 0.0599, 4: 0.0519, 5: 0.3134, 6: 0.0288, 7: 0.291, 8: 0.0699, 9: 0.0082, 10: 0.0077, 11: 0.0131, 12: 0.0186, 13: 0.0125, 14: 0.0186, 15: 0.018, 16: 0.0427})"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt_model.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_predictions = dtc_model.transform(test_data)\n",
    "rfc_predictions = rfc_model.transform(test_data)\n",
    "gbt_predictions = gbt_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictions and Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "A single decision tree had an accuracy of: 87.76%\n",
      "--------------------------------------------------------------------------------\n",
      "A random forest ensemble had an accuracy of: 97.03%\n",
      "--------------------------------------------------------------------------------\n",
      "A ensemble using GBT had an accuracy of: 95.68%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "binary_evaluator = BinaryClassificationEvaluator(labelCol=\"PrivateIndex\")\n",
    "\n",
    "\n",
    "dtc_be = binary_evaluator.evaluate(dtc_predictions)\n",
    "rfc_be = binary_evaluator.evaluate(rfc_predictions)\n",
    "gbt_be = binary_evaluator.evaluate(gbt_predictions)\n",
    "\n",
    "print('-'*80)\n",
    "print('A single decision tree had an accuracy of: {0:2.2f}%'.format(dtc_be*100))\n",
    "print('-'*80)\n",
    "print('A random forest ensemble had an accuracy of: {0:2.2f}%'.format(rfc_be*100))\n",
    "print('-'*80)\n",
    "print('A ensemble using GBT had an accuracy of: {0:2.2f}%'.format(gbt_be*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "A single decision tree had an accuracy of: 92.59%\n",
      "--------------------------------------------------------------------------------\n",
      "A random forest ensemble had an accuracy of: 92.59%\n",
      "--------------------------------------------------------------------------------\n",
      "A ensemble using GBT had an accuracy of: 92.59%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"PrivateIndex\", metricName = 'accuracy')\n",
    "\n",
    "\n",
    "dtc_acc = acc_evaluator.evaluate(dtc_predictions)\n",
    "rfc_acc = acc_evaluator.evaluate(rfc_predictions)\n",
    "gbt_acc = acc_evaluator.evaluate(gbt_predictions)\n",
    "\n",
    "print('-'*80)\n",
    "print('A single decision tree had an accuracy of: {0:2.2f}%'.format(dtc_acc*100))\n",
    "print('-'*80)\n",
    "print('A random forest ensemble had an accuracy of: {0:2.2f}%'.format(rfc_acc*100))\n",
    "print('-'*80)\n",
    "print('A ensemble using GBT had an accuracy of: {0:2.2f}%'.format(gbt_acc*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "spark = SparkSession.builder.appName('cluster').getOrCreate()\n",
    "\n",
    "data = spark.read.csv('E:\\\\SparkScala\\\\hack_data.csv',inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Session_Connection_Time: double (nullable = true)\n",
      " |-- Bytes Transferred: double (nullable = true)\n",
      " |-- Kali_Trace_Used: integer (nullable = true)\n",
      " |-- Servers_Corrupted: double (nullable = true)\n",
      " |-- Pages_Corrupted: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- WPM_Typing_Speed: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Session_Connection_Time=8.0, Bytes Transferred=391.09, Kali_Trace_Used=1, Servers_Corrupted=2.96, Pages_Corrupted=7.0, Location='Slovenia', WPM_Typing_Speed=72.37)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scaling data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Session_Connection_Time',\n",
       " 'Bytes Transferred',\n",
       " 'Kali_Trace_Used',\n",
       " 'Servers_Corrupted',\n",
       " 'Pages_Corrupted',\n",
       " 'Location',\n",
       " 'WPM_Typing_Speed']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Session_Connection_Time: double (nullable = true)\n",
      " |-- Bytes Transferred: double (nullable = true)\n",
      " |-- Kali_Trace_Used: integer (nullable = true)\n",
      " |-- Servers_Corrupted: double (nullable = true)\n",
      " |-- Pages_Corrupted: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- WPM_Typing_Speed: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=['Session_Connection_Time',\n",
    "'Bytes Transferred',\n",
    " 'Kali_Trace_Used',\n",
    " 'Servers_Corrupted',\n",
    " 'Pages_Corrupted',\n",
    " 'WPM_Typing_Speed'], outputCol='features')\n",
    "\n",
    "final_data=assembler.transform(data)\n",
    "\n",
    "final_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler (inputCol='features', outputCol='scaledFeatures')\n",
    "\n",
    "scaler_model = scaler.fit(final_data)\n",
    "\n",
    "cluster_final_data  = scaler_model.transform(final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KMeans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-8ae00dfbe8d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mkmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeaturesCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'scaledFeatures'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetSeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcluster_final_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcluster_final_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'prediction'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'KMeans' is not defined"
     ]
    }
   ],
   "source": [
    "kmeans2 = KMeans(featuresCol='scaledFeatures',k=2).setSeed(1)\n",
    "kmeans3 = KMeans(featuresCol='scaledFeatures',k=3).setSeed(1)\n",
    "\n",
    "model_k2 = kmeans2.fit(cluster_final_data)\n",
    "model_k3 = kmeans3.fit(cluster_final_data)\n",
    "\n",
    "model_k3.transform(cluster_final_data).groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|  167|\n",
      "|         0|  167|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_k2.transform(cluster_final_data).groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([2.99991988, 2.92319035, 1.05261534, 3.20390443, 4.51321315,\n",
       "        3.28474   ]),\n",
       " array([1.26023837, 1.31829808, 0.99280765, 1.36491885, 2.5625043 ,\n",
       "        5.26676612])]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centers_k2 = model_k2.clusterCenters()\n",
    "centers_k2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_k2.transform(cluster_final_data).select('prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender System - ALS Spark ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "spark = SparkSession.builder.appName('rec').getOrCreate()\n",
    "\n",
    "data = spark.read.csv('E:\\\\SparkScala\\\\movielens_ratings.csv',inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n",
      "|movieId|rating|userId|\n",
      "+-------+------+------+\n",
      "|      2|   3.0|     0|\n",
      "|      3|   1.0|     0|\n",
      "|      5|   2.0|     0|\n",
      "|      9|   4.0|     0|\n",
      "|     11|   1.0|     0|\n",
      "|     12|   2.0|     0|\n",
      "|     15|   1.0|     0|\n",
      "|     17|   1.0|     0|\n",
      "|     19|   1.0|     0|\n",
      "|     21|   1.0|     0|\n",
      "|     23|   1.0|     0|\n",
      "|     26|   3.0|     0|\n",
      "|     27|   1.0|     0|\n",
      "|     28|   1.0|     0|\n",
      "|     29|   1.0|     0|\n",
      "|     30|   1.0|     0|\n",
      "|     31|   1.0|     0|\n",
      "|     34|   1.0|     0|\n",
      "|     37|   1.0|     0|\n",
      "|     41|   2.0|     0|\n",
      "+-------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+\n",
      "|summary|           movieId|            rating|            userId|\n",
      "+-------+------------------+------------------+------------------+\n",
      "|  count|              1501|              1501|              1501|\n",
      "|   mean| 49.40572951365756|1.7741505662891406|14.383744170552964|\n",
      "| stddev|28.937034065088994| 1.187276166124803| 8.591040424293272|\n",
      "|    min|                 0|               1.0|                 0|\n",
      "|    max|                99|               5.0|                29|\n",
      "+-------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "training,test = data.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "als =  ALS(maxIter=5,regParam=0.01,userCol='userId',itemCol='movieId',ratingCol='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction and evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+\n",
      "|movieId|rating|userId|prediction|\n",
      "+-------+------+------+----------+\n",
      "|     31|   4.0|    12|-0.7838117|\n",
      "|     31|   3.0|     7| 1.7121273|\n",
      "|     31|   1.0|    24| 0.7028988|\n",
      "|     31|   3.0|    14| 2.2068293|\n",
      "|     31|   1.0|    18|  3.044821|\n",
      "|     85|   1.0|    13| 3.3847582|\n",
      "|     85|   1.0|     5| 0.7804052|\n",
      "|     85|   1.0|     2| 1.5486727|\n",
      "|     65|   1.0|    19| 0.9705255|\n",
      "|     65|   5.0|    23|0.80290216|\n",
      "|     65|   1.0|    24| 1.8640641|\n",
      "|     53|   3.0|    20|  1.303222|\n",
      "|     53|   5.0|     8| 3.9632707|\n",
      "|     53|   3.0|    14| 1.4016422|\n",
      "|     78|   1.0|    12|0.94908607|\n",
      "|     78|   1.0|     2|0.43352938|\n",
      "|     34|   4.0|     2| 3.1728232|\n",
      "|     81|   3.0|    26|0.23181152|\n",
      "|     81|   1.0|     1| 1.4737167|\n",
      "|     81|   1.0|    21| 1.6361859|\n",
      "+-------+------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 1.792724885582561\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single user recommendation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|movieId|userId|\n",
      "+-------+------+\n",
      "|     10|    11|\n",
      "|     12|    11|\n",
      "|     13|    11|\n",
      "|     27|    11|\n",
      "|     36|    11|\n",
      "|     38|    11|\n",
      "|     40|    11|\n",
      "|     41|    11|\n",
      "|     47|    11|\n",
      "|     62|    11|\n",
      "|     71|    11|\n",
      "|     80|    11|\n",
      "|     89|    11|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "single_user = test.filter(test['userId']==11).select(['movieId','userId'])\n",
    "single_user.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----------+\n",
      "|movieId|userId| prediction|\n",
      "+-------+------+-----------+\n",
      "|     27|    11|   5.276105|\n",
      "|     71|    11|   4.338571|\n",
      "|     38|    11|  2.5989215|\n",
      "|     12|    11|  1.5053954|\n",
      "|     36|    11|  1.2785971|\n",
      "|     89|    11|  1.1557362|\n",
      "|     13|    11|  1.0427175|\n",
      "|     40|    11| 0.70125484|\n",
      "|     10|    11| 0.20727384|\n",
      "|     80|    11| 0.11437428|\n",
      "|     41|    11|-0.26433903|\n",
      "|     47|    11| -1.0566144|\n",
      "|     62|    11| -3.1297584|\n",
      "+-------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recomendations = model.transform(single_user)\n",
    "\n",
    "recomendations.orderBy('prediction',ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            sentence|\n",
      "+---+--------------------+\n",
      "|  0|Hi I heard about ...|\n",
      "|  1|I wish Java could...|\n",
      "|  2|Logistic,regressi...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceDataFrame = spark.createDataFrame([(0, \"Hi I heard about Spark\"),\n",
    "    (1, \"I wish Java could use case classes\"),\n",
    "    (2, \"Logistic,regression,models,are,neat\")\n",
    "], [\"id\", \"sentence\"])\n",
    "\n",
    "sentenceDataFrame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+------+\n",
      "|sentence                           |words                                     |tokens|\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n",
      "|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
      "|Logistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |1     |\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# alternatively, pattern=\"\\\\w+\", gaps(False)\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "tokenized = tokenizer.transform(sentenceDataFrame)\n",
    "tokenized.select(\"sentence\", \"words\").withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+------+\n",
      "|sentence                           |words                                     |tokens|\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n",
      "|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
      "|Logistic,regression,models,are,neat|[logistic, regression, models, are, neat] |5     |\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n",
    "regexTokenized.select(\"sentence\", \"words\").withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StopWords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------+--------------------+\n",
      "|id |raw                         |filtered            |\n",
      "+---+----------------------------+--------------------+\n",
      "|0  |[I, saw, the, red, balloon] |[saw, red, balloon] |\n",
      "|1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n",
      "+---+----------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n",
    "    (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n",
    "], [\"id\", \"raw\"])\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\n",
    "remover.transform(sentenceData).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n-grams**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An n-gram is a sequence of nn tokens (typically words) for some integer nn. The NGram class can be used to transform input features into nn-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------+\n",
      "|ngrams                                                            |\n",
      "+------------------------------------------------------------------+\n",
      "|[Hi I, I heard, heard about, about Spark]                         |\n",
      "|[I wish, wish Java, Java could, could use, use case, case classes]|\n",
      "|[Logistic regression, regression models, models are, are neat]    |\n",
      "+------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "wordDataFrame = spark.createDataFrame([\n",
    "    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\n",
    "    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\n",
    "    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\n",
    "], [\"id\", \"words\"])\n",
    "\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
    "\n",
    "ngramDataFrame = ngram.transform(wordDataFrame)\n",
    "ngramDataFrame.select(\"ngrams\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term frequency-inverse document frequency (TF-IDF) is a feature vectorization method widely used in text mining to reflect the importance of a term to a document in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            sentence|\n",
      "+-----+--------------------+\n",
      "|  0.0|Hi I heard about ...|\n",
      "|  0.0|I wish Java could...|\n",
      "|  1.0|Logistic regressi...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0.0, \"Hi I heard about Spark\"),\n",
    "    (0.0, \"I wish Java could use case classes\"),\n",
    "    (1.0, \"Logistic regression models are neat\")\n",
    "], [\"label\", \"sentence\"])\n",
    "\n",
    "sentenceData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            sentence|               words|\n",
      "+-----+--------------------+--------------------+\n",
      "|  0.0|Hi I heard about ...|[hi, i, heard, ab...|\n",
      "|  0.0|I wish Java could...|[i, wish, java, c...|\n",
      "|  1.0|Logistic regressi...|[logistic, regres...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "wordsData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(20,[6,8,13,16],[...|\n",
      "|  0.0|(20,[0,2,7,13,15,...|\n",
      "|  1.0|(20,[3,4,6,11,19]...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "rescaledData.select(\"label\", \"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CountVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------------------------+\n",
      "|id |words          |features                 |\n",
      "+---+---------------+-------------------------+\n",
      "|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
      "|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
      "+---+---------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# Input data: Each row is a bag of words with a ID.\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"a b c\".split(\" \")),\n",
    "    (1, \"a b b c a\".split(\" \"))\n",
    "], [\"id\", \"words\"])\n",
    "\n",
    "# fit a CountVectorizerModel from the corpus.\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=3, minDF=2.0)\n",
    "\n",
    "model = cv.fit(df)\n",
    "\n",
    "result = model.transform(df)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|class|                text|\n",
      "+-----+--------------------+\n",
      "|  ham|Go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|\n",
      "| spam|Free entry in 2 a...|\n",
      "|  ham|U dun say so earl...|\n",
      "|  ham|Nah I don't think...|\n",
      "| spam|FreeMsg Hey there...|\n",
      "|  ham|Even my brother i...|\n",
      "|  ham|As per your reque...|\n",
      "| spam|WINNER!! As a val...|\n",
      "| spam|Had your mobile 1...|\n",
      "|  ham|I'm gonna be home...|\n",
      "| spam|SIX chances to wi...|\n",
      "| spam|URGENT! You have ...|\n",
      "|  ham|I've been searchi...|\n",
      "|  ham|I HAVE A DATE ON ...|\n",
      "| spam|XXXMobileMovieClu...|\n",
      "|  ham|Oh k...i'm watchi...|\n",
      "|  ham|Eh u remember how...|\n",
      "|  ham|Fine if thats th...|\n",
      "| spam|England v Macedon...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('nlp').getOrCreate()\n",
    "data = spark.read.csv(\"E:\\\\SparkScala\\\\SMSSpamCollection\",inferSchema=True,sep='\\t')\n",
    "data = data.withColumnRenamed('_c0','class').withColumnRenamed('_c1','text')\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+\n",
      "|class|                text|length|\n",
      "+-----+--------------------+------+\n",
      "|  ham|Go until jurong p...|   111|\n",
      "|  ham|Ok lar... Joking ...|    29|\n",
      "| spam|Free entry in 2 a...|   155|\n",
      "|  ham|U dun say so earl...|    49|\n",
      "|  ham|Nah I don't think...|    61|\n",
      "| spam|FreeMsg Hey there...|   147|\n",
      "|  ham|Even my brother i...|    77|\n",
      "|  ham|As per your reque...|   160|\n",
      "| spam|WINNER!! As a val...|   157|\n",
      "| spam|Had your mobile 1...|   154|\n",
      "|  ham|I'm gonna be home...|   109|\n",
      "| spam|SIX chances to wi...|   136|\n",
      "| spam|URGENT! You have ...|   155|\n",
      "|  ham|I've been searchi...|   196|\n",
      "|  ham|I HAVE A DATE ON ...|    35|\n",
      "| spam|XXXMobileMovieClu...|   149|\n",
      "|  ham|Oh k...i'm watchi...|    26|\n",
      "|  ham|Eh u remember how...|    81|\n",
      "|  ham|Fine if thats th...|    56|\n",
      "| spam|England v Macedon...|   155|\n",
      "+-----+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length\n",
    "\n",
    "data = data.withColumn('length',length(data['text']))\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|class|      avg(length)|\n",
      "+-----+-----------------+\n",
      "|  ham|71.45431945307645|\n",
      "| spam|138.6706827309237|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy('class').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer,StopWordsRemover, CountVectorizer,IDF,StringIndexer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\n",
    "stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
    "count_vec = CountVectorizer(inputCol='stop_tokens',outputCol='c_vec')\n",
    "idf = IDF(inputCol=\"c_vec\", outputCol=\"tf_idf\")\n",
    "ham_spam_to_num = StringIndexer(inputCol='class',outputCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector\n",
    "\n",
    "clean_up = VectorAssembler(inputCols=['tf_idf','length'],outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "nb = NaiveBayes()\n",
    "\n",
    "#Pipeline\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "data_prep_pipe = Pipeline(stages=[ham_spam_to_num,tokenizer,stopremove,count_vec,idf,clean_up])\n",
    "\n",
    "cleaner = data_prep_pipe.fit(data)\n",
    "\n",
    "clean_data = cleaner.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- class: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- length: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_data = clean_data.select(['label','features'])\n",
    "\n",
    "(training,testing) = clean_data.randomSplit([0.7,0.3])\n",
    "\n",
    "spam_predictor = nb.fit(training)\n",
    "\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(13424,[0,1,2,7,8...|[-806.91554692136...|[1.0,5.0313921217...|       0.0|\n",
      "|  0.0|(13424,[0,1,2,41,...|[-1060.0763099804...|[1.0,1.0010834603...|       0.0|\n",
      "|  0.0|(13424,[0,1,3,9,1...|[-591.60623033257...|[0.99999999996580...|       0.0|\n",
      "|  0.0|(13424,[0,1,7,8,1...|[-1170.2132665604...|[1.0,4.4268199765...|       0.0|\n",
      "|  0.0|(13424,[0,1,7,15,...|[-658.14673929182...|[1.0,1.1765181807...|       0.0|\n",
      "|  0.0|(13424,[0,1,9,14,...|[-558.36937489931...|[1.0,3.8397344066...|       0.0|\n",
      "|  0.0|(13424,[0,1,9,14,...|[-558.36937489931...|[1.0,3.8397344066...|       0.0|\n",
      "|  0.0|(13424,[0,1,14,18...|[-1379.6926927541...|[1.0,7.0657612800...|       0.0|\n",
      "|  0.0|(13424,[0,1,17,19...|[-800.14645937540...|[1.0,8.8634507208...|       0.0|\n",
      "|  0.0|(13424,[0,1,20,27...|[-982.65331375717...|[0.99999999991856...|       0.0|\n",
      "|  0.0|(13424,[0,1,24,31...|[-340.67318725509...|[1.0,7.9673034148...|       0.0|\n",
      "|  0.0|(13424,[0,1,46,17...|[-1135.6522255675...|[3.63300322256846...|       1.0|\n",
      "|  0.0|(13424,[0,1,72,10...|[-682.51311556100...|[1.0,7.4218937891...|       0.0|\n",
      "|  0.0|(13424,[0,1,416,6...|[-318.17470624156...|[0.99999998942027...|       0.0|\n",
      "|  0.0|(13424,[0,1,498,5...|[-316.90078495536...|[0.99999999999979...|       0.0|\n",
      "|  0.0|(13424,[0,1,874,1...|[-96.142020498086...|[0.99999996614519...|       0.0|\n",
      "|  0.0|(13424,[0,1,874,1...|[-97.823839361412...|[0.99999997430922...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,5,1...|[-2491.5363557269...|[1.0,3.8309388266...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,44,...|[-1922.6490385842...|[1.0,8.6798997344...|       0.0|\n",
      "|  0.0|(13424,[0,2,7,8,1...|[-453.46278583796...|[1.0,7.5702354988...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results = spam_predictor.transform(testing)\n",
    "test_results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting spam was: 0.9257039277582748\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "print(\"Accuracy of model at predicting spam was: {}\".format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
